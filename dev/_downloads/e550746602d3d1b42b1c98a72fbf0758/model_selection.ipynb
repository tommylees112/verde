{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nModel Selection\n===============\n\nThe Green's functions based interpolations in Verde are all linear regressions under the\nhood. This means that we can use some of the same tactics from\n:mod:`sklearn.model_selection` to evaluate our interpolator's performance. Once we have\na quantified measure of the quality of a given fitted gridder, we can use it to tune the\ngridder's parameters, like ``damping`` for a :class:`~verde.Spline`.\n\nVerde provides adaptations of common scikit-learn tools to work better with spatial\ndata. Let's use these tools to evaluate and tune a :class:`~verde.Spline` to grid our\nsample air temperature data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport itertools\nimport pyproj\nimport verde as vd\n\ndata = vd.datasets.fetch_texas_wind()\n\n# Use Mercator projection because Spline is a Cartesian gridder\nprojection = pyproj.Proj(proj=\"merc\", lat_ts=data.latitude.mean())\nproj_coords = projection(data.longitude.values, data.latitude.values)\n\nregion = vd.get_region((data.longitude, data.latitude))\n# The desired grid spacing in degrees (converted to meters using 1 degree approx. 111km)\nspacing = 15 / 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting the data\n------------------\n\nWe can't evaluate a gridder on the data that went into fitting it. The true test of a\nmodel is if it can correctly predict data that it hasn't seen before. scikit-learn has\nthe :func:`sklearn.model_selection.train_test_split` function to separate a dataset\ninto two parts: one for fitting the model (called *training* data) and a separate one\nfor evaluating the model (called *testing* data). Using it with spatial data would\ninvolve some tedious array conversions so Verde implements\n:func:`verde.train_test_split` which does the same thing but takes coordinates and\ndata arrays instead.\n\nThe split is done randomly so we specify a seed for the random number generator to\nguarantee that we'll get the same result every time we run this example. You probably\ndon't want to do that for real data. We'll keep 30% of the data to use for testing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train, test = vd.train_test_split(\n    proj_coords, data.air_temperature_c, test_size=0.3, random_state=0\n)\nprint(train)\nprint(test)\n\nplt.figure(figsize=(8, 6))\nax = plt.axes()\nax.set_title(\"Air temperature measurements for Texas\")\nax.plot(train[0][0], train[0][1], \".r\", label=\"train\")\nax.plot(test[0][0], test[0][1], \".b\", label=\"test\")\nax.legend()\nax.set_aspect(\"equal\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The returned ``train`` and ``test`` arguments are each tuples with the coordinates (in\na tuple) and a data array. They are in a format that can be easily passed to the\n:meth:`~verde.base.BaseGridder.fit` method of most gridders using Python's argument\nexpansion using the ``*`` symbol.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spline = vd.Spline()\nspline.fit(*train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the gridded result to see what it looks like. We'll mask out grid points\nthat are too far from any given data point.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask = vd.distance_mask(\n    (data.longitude, data.latitude),\n    maxdist=3 * spacing * 111e3,\n    coordinates=vd.grid_coordinates(region, spacing=spacing),\n    projection=projection,\n)\ngrid = spline.grid(\n    region=region,\n    spacing=spacing,\n    projection=projection,\n    dims=[\"latitude\", \"longitude\"],\n    data_names=[\"temperature\"],\n).where(mask)\n\nplt.figure(figsize=(8, 6))\nax = plt.axes(projection=ccrs.Mercator())\nax.set_title(\"Gridded temperature\")\npc = grid.temperature.plot.pcolormesh(\n    ax=ax,\n    cmap=\"plasma\",\n    transform=ccrs.PlateCarree(),\n    add_colorbar=False,\n    add_labels=False,\n)\nplt.colorbar(pc).set_label(\"C\")\nax.plot(data.longitude, data.latitude, \".k\", markersize=1, transform=ccrs.PlateCarree())\nvd.datasets.setup_texas_wind_map(ax)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scoring\n--------\n\nGridders in Verde implement the :meth:`~verde.base.BaseGridder.score` method that\ncalculates the `R\u00b2 coefficient of determination\n<https://en.wikipedia.org/wiki/Coefficient_of_determination>`__\nfor a given comparison dataset (``test`` in our case). The R\u00b2 score is at most 1,\nmeaning a perfect prediction, but has no lower bound.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score = spline.score(*test)\nprint(\"R\u00b2 score:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's a good score meaning that our gridder is able to accurately predict data that\nwasn't used in the gridding algorithm.\n\nOnce caveat for this score is that it is highly dependent on the particular split that\nwe made. Changing the random number generator seed in :func:`verde.train_test_split`\nwill result in a different score.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Use 1 as a seed instead of 0\ntrain_other, test_other = vd.train_test_split(\n    proj_coords, data.air_temperature_c, test_size=0.3, random_state=1\n)\nprint(\"R\u00b2 score with seed 1:\", spline.fit(*train_other).score(*test_other))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A more robust way of scoring the gridders is to use function\n:func:`verde.cross_val_score`, which (by default) uses a `k-fold cross-validation\n<https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation>`__.\nIt will split the data *k* times and return the score on each *fold*. We can then take\na mean of these scores.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = vd.cross_val_score(spline, proj_coords, data.air_temperature_c)\nprint(\"k-fold scores:\", scores)\nprint(\"Mean score:\", np.mean(scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That is not a very good score so clearly the default arguments for\n:class:`~verde.Spline` aren't suitable for this dataset. We could try different\ncombinations manually until we get a good score. A better way is to do this\nautomatically.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tuning\n------\n\n:class:`~verde.Spline` has many parameters that can be set to modify the final result.\nMainly the ``damping`` regularization parameter and the ``mindist`` \"fudge factor\"\nwhich smooths the solution. Would changing the default values give us a better score?\n\nWe can answer these questions by changing the values in our ``spline`` and\nre-evaluating the model score repeatedly for different values of these parameters.\nLet's test the following combinations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dampings = [None, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\nmindists = [5e3, 10e3, 25e3, 50e3, 75e3, 100e3]\n\n# Use itertools to create a list with all combinations of parameters to test\nparameter_sets = [\n    dict(damping=combo[0], mindist=combo[1])\n    for combo in itertools.product(dampings, mindists)\n]\nprint(\"Number of combinations:\", len(parameter_sets))\nprint(\"Combinations:\", parameter_sets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can loop over the combinations and collect the scores for each parameter set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = []\nfor params in parameter_sets:\n    spline.set_params(**params)\n    score = np.mean(vd.cross_val_score(spline, proj_coords, data.air_temperature_c))\n    scores.append(score)\nprint(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The largest score will yield the best parameter combination.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best = np.argmax(scores)\nprint(\"Best score:\", scores[best])\nprint(\"Best parameters:\", parameter_sets[best])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That is a big improvement over our previous score!\n\nWe can now configure our spline with the best configuration and re-fit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spline.set_params(**parameter_sets[best])\nspline.fit(proj_coords, data.air_temperature_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can make a grid with the best configuration to see how it compares to our\nprevious result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grid_best = spline.grid(\n    region=region,\n    spacing=spacing,\n    projection=projection,\n    dims=[\"latitude\", \"longitude\"],\n    data_names=[\"temperature\"],\n).where(mask)\n\nplt.figure(figsize=(14, 8))\nfor i, title, grd in zip(range(2), [\"Defaults\", \"Tuned\"], [grid, grid_best]):\n    ax = plt.subplot(1, 2, i + 1, projection=ccrs.Mercator())\n    ax.set_title(title)\n    pc = grd.temperature.plot.pcolormesh(\n        ax=ax,\n        cmap=\"plasma\",\n        transform=ccrs.PlateCarree(),\n        vmin=data.air_temperature_c.min(),\n        vmax=data.air_temperature_c.max(),\n        add_colorbar=False,\n        add_labels=False,\n    )\n    plt.colorbar(pc, orientation=\"horizontal\", aspect=50, pad=0.05).set_label(\"C\")\n    ax.plot(\n        data.longitude, data.latitude, \".k\", markersize=1, transform=ccrs.PlateCarree()\n    )\n    vd.datasets.setup_texas_wind_map(ax)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that, for sparse data like these, smoother models tend to be better predictors.\nThis is a sign that you should probably not trust many of the short wavelength\nfeatures that we get from the defaults.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}